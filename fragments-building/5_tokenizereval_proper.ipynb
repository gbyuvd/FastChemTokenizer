{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fe1dc38",
   "metadata": {},
   "source": [
    "# Evaluation vs Optix SELFIE WordLevel Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3c643f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️  1000 encodes in 0.0021 seconds\n"
     ]
    }
   ],
   "source": [
    "from FastChemTokenizer import FastChemTokenizer\n",
    "tokenizer = FastChemTokenizer.from_pretrained(\"../chemtok\")\n",
    "\n",
    "test_text = \"CCO\" * 500  # Long repeated string to stress test\n",
    "import time\n",
    "# Warm up cache\n",
    "tokenizer.encode(test_text)\n",
    "# Benchmark\n",
    "start = time.perf_counter()\n",
    "for _ in range(1000):\n",
    "    tokenizer.encode(test_text)\n",
    "end = time.perf_counter()\n",
    "print(f\"⏱️  1000 encodes in {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b4291e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../comb_smi.csv in chunks of 50,000 rows...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e8d872f88684d2a9f09f0361fd62aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing chunks:   0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (551 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TOKENIZER PERFORMANCE COMPARISON RESULTS\n",
      "============================================================\n",
      "Total texts processed: 2,700,000\n",
      "Chunk size: 50,000 | Columns: ['SMILES']\n",
      "\n",
      "[Tokenizer 1: chemfie-experiment-1]\n",
      "  Avg time per text: 0.0938 ms\n",
      "  Avg tokens per text: 50.57\n",
      "  UNK token rate: 0.0000%\n",
      "  Peak memory usage: 387.43 MB\n",
      "\n",
      "[Tokenizer 2: new prototype tokenizer]\n",
      "  Avg time per text: 0.0625 ms\n",
      "  Avg tokens per text: 21.49\n",
      "  UNK token rate: 0.0000%\n",
      "  Peak memory usage: 15.34 MB\n",
      "\n",
      "[Comparison Summary]\n",
      "Tokenizer 2 is 1.50x faster\n",
      "Tokenizer 1 produces 2.35x more tokens\n",
      "Tokenizer 2 has 0.0000% lower UNK rate\n",
      "\n",
      "⚠️ WARNING: UNK tokens detected! Check if your tokenizers were trained on this data.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "# Load tokenizers with proper naming\n",
    "#tok1 = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "tok1 = AutoTokenizer.from_pretrained(\"smostafanejad/gen-mlm-cismi-bert-wordpiece\", torch_dtype=\"auto\")  \n",
    "\n",
    "# Reconstruct tokenizer\n",
    "tok2 = FastChemTokenizer.from_pretrained(\"../chemtok\")\n",
    "\n",
    "# Get UNK token IDs for counting\n",
    "unk_id1 = tok1.unk_token_id\n",
    "unk_id2 = tok2.unk_token_id\n",
    "\n",
    "# Initialize metrics\n",
    "metrics = {\n",
    "    \"tok1\": {\"total_time\": 0, \"total_tokens\": 0, \"unk_count\": 0, \"max_memory\": 0},\n",
    "    \"tok2\": {\"total_time\": 0, \"total_tokens\": 0, \"unk_count\": 0, \"max_memory\": 0},\n",
    "}\n",
    "\n",
    "# Read CSV in chunks\n",
    "chunksize = 50_000\n",
    "data_path = \"../comb_smi.csv\"\n",
    "col_name = \"SMILES\"\n",
    "\n",
    "# Count total number of rows for tqdm if desired (optional for more accuracy)\n",
    "total_rows = sum(1 for _ in open(data_path)) - 1  # -1 for header\n",
    "total_chunks = (total_rows + chunksize - 1) // chunksize\n",
    "\n",
    "print(f\"Processing {data_path} in chunks of {chunksize:,} rows...\")\n",
    "\n",
    "# tqdm progress bar over the chunks\n",
    "with tqdm(pd.read_csv(data_path, chunksize=chunksize, usecols=[col_name]), desc=\"Processing chunks\", total=total_chunks) as pbar:\n",
    "    for chunk_idx, chunk in enumerate(pbar):\n",
    "        selfies = chunk[col_name].tolist()\n",
    "        total_texts = len(selfies)\n",
    "\n",
    "        # Process Tokenizer 1\n",
    "        process = psutil.Process(os.getpid())\n",
    "        mem_before = process.memory_info().rss / 1024**2  # MB\n",
    "\n",
    "        start = time.perf_counter()\n",
    "        encoded1 = tok1.batch_encode_plus(\n",
    "            selfies,\n",
    "            padding=False,\n",
    "            truncation=False,\n",
    "            return_attention_mask=False,\n",
    "            return_token_type_ids=False\n",
    "        )\n",
    "        end = time.perf_counter()\n",
    "\n",
    "        mem_after = process.memory_info().rss / 1024**2\n",
    "        metrics[\"tok1\"][\"max_memory\"] = max(metrics[\"tok1\"][\"max_memory\"], mem_after - mem_before)\n",
    "        metrics[\"tok1\"][\"total_time\"] += (end - start)\n",
    "        metrics[\"tok1\"][\"total_tokens\"] += sum(len(ids) for ids in encoded1[\"input_ids\"])\n",
    "        metrics[\"tok1\"][\"unk_count\"] += sum(ids.count(unk_id1) for ids in encoded1[\"input_ids\"])\n",
    "\n",
    "        # Process Tokenizer 2\n",
    "        mem_before = process.memory_info().rss / 1024**2\n",
    "        start = time.perf_counter()\n",
    "        encoded2 = tok2.batch_encode_plus(\n",
    "            selfies,\n",
    "            padding=False,\n",
    "            truncation=False,\n",
    "            return_attention_mask=False,\n",
    "            return_token_type_ids=False\n",
    "        )\n",
    "        end = time.perf_counter()\n",
    "\n",
    "        mem_after = process.memory_info().rss / 1024**2\n",
    "        metrics[\"tok2\"][\"max_memory\"] = max(metrics[\"tok2\"][\"max_memory\"], mem_after - mem_before)\n",
    "        metrics[\"tok2\"][\"total_time\"] += (end - start)\n",
    "        metrics[\"tok2\"][\"total_tokens\"] += sum(len(ids) for ids in encoded2[\"input_ids\"])\n",
    "        metrics[\"tok2\"][\"unk_count\"] += sum(ids.count(unk_id2) for ids in encoded2[\"input_ids\"])\n",
    "        pbar.set_postfix(chunk=chunk_idx + 1)\n",
    "\n",
    "# Final metrics\n",
    "total_texts = (chunk_idx + 1) * chunksize\n",
    "avg_tokens_tok1 = metrics[\"tok1\"][\"total_tokens\"] / total_texts\n",
    "avg_tokens_tok2 = metrics[\"tok2\"][\"total_tokens\"] / total_texts\n",
    "\n",
    "unk_rate_tok1 = (metrics[\"tok1\"][\"unk_count\"] / metrics[\"tok1\"][\"total_tokens\"]) * 100 if metrics[\"tok1\"][\"total_tokens\"] > 0 else 0\n",
    "unk_rate_tok2 = (metrics[\"tok2\"][\"unk_count\"] / metrics[\"tok2\"][\"total_tokens\"]) * 100 if metrics[\"tok2\"][\"total_tokens\"] > 0 else 0\n",
    "\n",
    "time_per_text_tok1 = (metrics[\"tok1\"][\"total_time\"] / total_texts) * 1000  # ms\n",
    "time_per_text_tok2 = (metrics[\"tok2\"][\"total_time\"] / total_texts) * 1000  # ms\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TOKENIZER PERFORMANCE COMPARISON RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total texts processed: {total_texts:,}\")\n",
    "print(f\"Chunk size: {chunksize:,} | Columns: ['{col_name}']\")\n",
    "\n",
    "print(\"\\n[Tokenizer 1: chemfie-experiment-1]\")\n",
    "print(f\"  Avg time per text: {time_per_text_tok1:.4f} ms\")\n",
    "print(f\"  Avg tokens per text: {avg_tokens_tok1:.2f}\")\n",
    "print(f\"  UNK token rate: {unk_rate_tok1:.4f}%\")\n",
    "print(f\"  Peak memory usage: {metrics['tok1']['max_memory']:.2f} MB\")\n",
    "\n",
    "print(\"\\n[Tokenizer 2: new prototype tokenizer]\")\n",
    "print(f\"  Avg time per text: {time_per_text_tok2:.4f} ms\")\n",
    "print(f\"  Avg tokens per text: {avg_tokens_tok2:.2f}\")\n",
    "print(f\"  UNK token rate: {unk_rate_tok2:.4f}%\")\n",
    "print(f\"  Peak memory usage: {metrics['tok2']['max_memory']:.2f} MB\")\n",
    "\n",
    "print(\"\\n[Comparison Summary]\")\n",
    "print(f\"Tokenizer 2 is {time_per_text_tok1 / time_per_text_tok2:.2f}x faster\")\n",
    "print(f\"Tokenizer 1 produces {avg_tokens_tok1 / avg_tokens_tok2:.2f}x more tokens\")\n",
    "print(f\"Tokenizer 2 has {unk_rate_tok1 - unk_rate_tok2:.4f}% lower UNK rate\")\n",
    "\n",
    "if unk_rate_tok1 > 0 or unk_rate_tok2 > 0:\n",
    "    print(\"\\n⚠️ WARNING: UNK tokens detected! Check if your tokenizers were trained on this data.\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5733c8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TOKENIZER PERFORMANCE COMPARISON RESULTS\n",
      "============================================================\n",
      "Total texts processed: 2,700,000\n",
      "Chunk size: 50,000 | Columns: ['SMILES']\n",
      "\n",
      "[Tokenizer 1: chemfie-experiment-1]\n",
      "  Avg time per text: 0.0938 ms\n",
      "  Avg sequence length: 50.57 tokens\n",
      "  UNK token rate: 0.0000%\n",
      "  Peak memory usage: 387.43 MB\n",
      "\n",
      "[Tokenizer 2: new prototype tokenizer]\n",
      "  Avg time per text: 0.0625 ms\n",
      "  Avg sequence length: 21.49 tokens\n",
      "  UNK token rate: 0.0000%\n",
      "  Peak memory usage: 15.34 MB\n",
      "\n",
      "[Comparison Summary]\n",
      "Tokenizer 2 is 1.50x faster\n",
      "Tokenizer 1 produces 2.35x more tokens\n",
      "Tokenizer 2 has 0.0000% lower UNK rate\n",
      "\n",
      "⚠️ WARNING: UNK tokens detected! Check if your tokenizers were trained on this data.\n",
      "============================================================\n",
      "  Throughput Tok1: 10,658 texts/sec\n",
      "  Throughput Tok2: 15,995 texts/sec\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TOKENIZER PERFORMANCE COMPARISON RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total texts processed: {total_texts:,}\")\n",
    "print(f\"Chunk size: {chunksize:,} | Columns: ['{col_name}']\")\n",
    "\n",
    "print(\"\\n[Tokenizer 1: chemfie-experiment-1]\")\n",
    "print(f\"  Avg time per text: {time_per_text_tok1:.4f} ms\")\n",
    "print(f\"  Avg sequence length: {avg_tokens_tok1:.2f} tokens\")\n",
    "print(f\"  UNK token rate: {unk_rate_tok1:.4f}%\")\n",
    "print(f\"  Peak memory usage: {metrics['tok1']['max_memory']:.2f} MB\")\n",
    "\n",
    "print(\"\\n[Tokenizer 2: new prototype tokenizer]\")\n",
    "print(f\"  Avg time per text: {time_per_text_tok2:.4f} ms\")\n",
    "print(f\"  Avg sequence length: {avg_tokens_tok2:.2f} tokens\")\n",
    "print(f\"  UNK token rate: {unk_rate_tok2:.4f}%\")\n",
    "print(f\"  Peak memory usage: {metrics['tok2']['max_memory']:.2f} MB\")\n",
    "\n",
    "print(\"\\n[Comparison Summary]\")\n",
    "speed_ratio = time_per_text_tok1 / time_per_text_tok2\n",
    "if speed_ratio > 1:\n",
    "    print(f\"Tokenizer 2 is {speed_ratio:.2f}x faster\")\n",
    "else:\n",
    "    print(f\"Tokenizer 2 is {1/speed_ratio:.2f}x slower\")\n",
    "\n",
    "print(f\"Tokenizer 1 produces {avg_tokens_tok1 / avg_tokens_tok2:.2f}x more tokens\")\n",
    "print(f\"Tokenizer 2 has {unk_rate_tok1 - unk_rate_tok2:.4f}% lower UNK rate\")\n",
    "\n",
    "if unk_rate_tok1 > 0 or unk_rate_tok2 > 0:\n",
    "    print(\"\\n⚠️ WARNING: UNK tokens detected! Check if your tokenizers were trained on this data.\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "throughput_tok1 = total_texts / metrics[\"tok1\"][\"total_time\"]  # texts/sec\n",
    "throughput_tok2 = total_texts / metrics[\"tok2\"][\"total_time\"]\n",
    "\n",
    "print(f\"  Throughput Tok1: {throughput_tok1:,.0f} texts/sec\")\n",
    "print(f\"  Throughput Tok2: {throughput_tok2:,.0f} texts/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "101f7a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️  1000 encodes in 0.5965 seconds\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model = AutoTokenizer.from_pretrained(\"smostafanejad/gen-mlm-cismi-bert-wordpiece\", torch_dtype=\"auto\")  \n",
    "\n",
    "test_text = \"CCO\" * 500  # Long repeated string to stress test\n",
    "import time\n",
    "# Warm up cache\n",
    "model.encode(test_text)\n",
    "# Benchmark\n",
    "start = time.perf_counter()\n",
    "for _ in range(1000):\n",
    "    model.encode(test_text)\n",
    "end = time.perf_counter()\n",
    "print(f\"⏱️  1000 encodes in {end - start:.4f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
